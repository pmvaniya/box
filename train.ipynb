{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install tiktoken\n",
    "!pip install torch\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/refs/heads/master/data/tinyshakespeare/input.txt\n",
    "!wget https://raw.githubusercontent.com/pranav-vaniya/box/refs/heads/main/data/token_ids.json\n",
    "\n",
    "for i in range(10):\n",
    "    url = f\"https://raw.githubusercontent.com/pranav-vaniya/owt-dataset-01/refs/heads/main/array{i:04}.npy\"\n",
    "    !wget {url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import requests\n",
    "import tarfile\n",
    "import json\n",
    "import math\n",
    "import tiktoken\n",
    "import inspect\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from pathlib import Path\n",
    "from shutil import rmtree\n",
    "from dataclasses import dataclass\n",
    "from torch.nn import functional as F\n",
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURR_DIR = Path.cwd()  # Current working directory for the notebook\n",
    "DATA_DIR = CURR_DIR / \"data\"\n",
    "OWT_DIR = DATA_DIR / \"OpenWebText\"\n",
    "EXTRACT_DIR = DATA_DIR / \"extracted\"\n",
    "CONVERT_DIR = DATA_DIR / \"converted\"\n",
    "TOKEN_ID_FILE = \"token_ids.json\"\n",
    "\n",
    "if os.path.exists(DATA_DIR) == False:\n",
    "    os.mkdir(DATA_DIR)\n",
    "\n",
    "stoi = json.load(open(TOKEN_ID_FILE, \"r\"))\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "max_token_length = max([len(s) for s, i in stoi.items()])\n",
    "shard_size = 100_000_000  # 100M tokens per shard\n",
    "\n",
    "print(\"vocab size:\", len(stoi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text):\n",
    "    text_length = len(text)\n",
    "    encoded_text = []\n",
    "    # encoded_text.append(stoi[\"<|S|>\"])\n",
    "    i = 0\n",
    "\n",
    "    # Greedy tokenization\n",
    "    while i < text_length:\n",
    "        matched = False\n",
    "\n",
    "        # Ensure that we only check lengths up to max_token_length\n",
    "        max_len = min(max_token_length, text_length - i)\n",
    "\n",
    "        for length in range(max_len, 0, -1):\n",
    "            token = text[i : i + length]\n",
    "\n",
    "            if token in stoi:\n",
    "                encoded_text.append(stoi[token])\n",
    "                i += length\n",
    "                matched = True\n",
    "                break\n",
    "\n",
    "        if not matched:\n",
    "            encoded_text.append(stoi[\"<|OoV|>\"])\n",
    "            i += 1\n",
    "\n",
    "    # encoded_text.append(stoi[\"<|E|>\"])\n",
    "\n",
    "    return encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tokens(tokens):\n",
    "    return \"\".join([itos[id] for id in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_owt(des_dir) -> None:\n",
    "    if os.path.exists(des_dir):\n",
    "        rmtree(des_dir)\n",
    "\n",
    "    os.mkdir(des_dir)\n",
    "\n",
    "    total_subset_files = 21\n",
    "    counter = 1\n",
    "\n",
    "    links = [\n",
    "        f\"https://huggingface.co/datasets/Skylion007/openwebtext/resolve/main/subsets/urlsf_subset{i:02}.tar?download=true\"\n",
    "        for i in range(total_subset_files)\n",
    "    ]\n",
    "\n",
    "    for link in links:\n",
    "        name = link.split(\"/\")[-1].split(\"?\")[0]\n",
    "        download_path = des_dir / name\n",
    "        print(f\"Downloading {name} (file {counter} of {total_subset_files})\")\n",
    "        counter += 1\n",
    "\n",
    "        try:\n",
    "            response = requests.get(link)\n",
    "            open(download_path, \"wb\").write(response.content)\n",
    "        except:\n",
    "            print(f\"Internet?\")\n",
    "            exit()\n",
    "\n",
    "# download_owt(des_dir=OWT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_owt(src_dir, des_dir) -> None:\n",
    "    tars = sorted(os.listdir(src_dir))\n",
    "    subtar_dir = des_dir / \"openwebtext\"\n",
    "    counter = 0\n",
    "\n",
    "    if os.path.exists(des_dir):\n",
    "        rmtree(des_dir)\n",
    "\n",
    "    for tar in tars[:2]:\n",
    "        print(\"extracting\", tar)\n",
    "        tar_path = OWT_DIR / tar\n",
    "        extraction_path = des_dir / f\"subset{counter:02}\"\n",
    "\n",
    "        with tarfile.open(tar_path, \"r\") as archive:\n",
    "            archive.extractall(path=des_dir, filter=\"tar\")\n",
    "            subtars = os.listdir(subtar_dir)\n",
    "\n",
    "            for subtar in subtars:\n",
    "                subtar_path = subtar_dir / subtar\n",
    "\n",
    "                with tarfile.open(subtar_path, \"r\") as subarchive:\n",
    "                    subarchive.extractall(path=extraction_path, filter=\"tar\")\n",
    "\n",
    "            rmtree(subtar_dir)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "# extract_owt(src_dir=OWT_DIR, des_dir=EXTRACT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_owt(src_dir, des_dir):\n",
    "    if os.path.exists(des_dir):\n",
    "        rmtree(des_dir)\n",
    "\n",
    "    os.mkdir(des_dir)\n",
    "\n",
    "    print(\"starting conversion\")\n",
    "    textfile_dirs = sorted(os.listdir(src_dir))\n",
    "    np_array = np.zeros(shard_size, dtype=np.uint16)\n",
    "    token_counter = 0\n",
    "    shard_counter = 0\n",
    "\n",
    "    for textfile_dir in textfile_dirs:\n",
    "        textfile_dir_path = src_dir / textfile_dir\n",
    "        textfiles = os.listdir(textfile_dir_path)\n",
    "\n",
    "        for textfile in textfiles:\n",
    "            textfile_path = textfile_dir_path / textfile\n",
    "            text = open(textfile_path, \"r\").read()\n",
    "            encoded = encode_text(text)\n",
    "            encoded_length = len(encoded)\n",
    "\n",
    "            if token_counter + encoded_length < shard_size:\n",
    "                np_array[token_counter : token_counter + encoded_length] = encoded\n",
    "                token_counter += encoded_length\n",
    "\n",
    "            else:\n",
    "                np_array[token_counter:shard_size] = encoded[\n",
    "                    : (shard_size - token_counter)\n",
    "                ]\n",
    "                np.save(des_dir / f\"array{shard_counter:04}.npy\", np_array)\n",
    "                print(f\"saved array{shard_counter:04}.npy\")\n",
    "\n",
    "                token_counter = 0\n",
    "                shard_counter += 1\n",
    "                np_array = np.zeros(shard_size, dtype=np.uint16)\n",
    "\n",
    "# convert_owt(src_dir=EXTRACT_DIR, des_dir=CONVERT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 1000\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
    "                1, 1, config.block_size, config.block_size\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()  # batch size, sequence length, n_embd\n",
    "\n",
    "        qkv = self.c_attn(x)  # query, key, values\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "        # att = F.softmax(att, dim=-1)\n",
    "        # y = att @ v\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "                wpe=nn.Embedding(config.block_size, config.n_embd),\n",
    "                h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "                ln_f=nn.LayerNorm(config.n_embd),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device):\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
    "            {\"params\": nodecay_params, \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "\n",
    "        print(\n",
    "            f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\"\n",
    "        )\n",
    "        print(\n",
    "            f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params} parameters\"\n",
    "        )\n",
    "\n",
    "        fused_available = \"fused\" in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and \"cuda\" in device\n",
    "\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused\n",
    "        )\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "\n",
    "            if hasattr(module, \"NANOGPT_SCALE_INIT\"):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.size()\n",
    "\n",
    "        assert T <= self.config.block_size, f\"cannot forward sequence length {T}\"\n",
    "\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        assert model_type in {\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"}\n",
    "\n",
    "        print(f\"loading weights from pretrained gpt: {model_type}\")\n",
    "\n",
    "        config_args = {\n",
    "            \"gpt2\": dict(n_layer=12, n_head=12, n_embd=768),\n",
    "            \"gpt2-medium\": dict(n_layer=24, n_head=16, n_embd=1024),\n",
    "            \"gpt2-large\": dict(n_layer=36, n_head=20, n_embd=1280),\n",
    "            \"gpt2-xl\": dict(n_layer=48, n_head=25, n_embd=1600),\n",
    "        }[model_type]\n",
    "\n",
    "        config_args[\"vocab_size\"] = 50257\n",
    "        config_args[\"block_size\"] = 1024\n",
    "\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith(\".attn.bias\")]\n",
    "\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith(\".attn.masked_bias\")]\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith(\".attn.bias\")]\n",
    "\n",
    "        transposed = [\n",
    "            \"attn.c_attn.weight\",\n",
    "            \"attn.c_proj.weight\",\n",
    "            \"mlp.c_fc.weight\",\n",
    "            \"mlp.c_proj.weight\",\n",
    "        ]\n",
    "\n",
    "        assert len(sd_keys_hf) == len(\n",
    "            sd_keys\n",
    "        ), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "\n",
    "            else:\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokens(filename):\n",
    "    npt = np.load(filename)\n",
    "    ptt = torch.tensor(npt, dtype=torch.long)\n",
    "    return ptt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, split):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        assert split in {\"train\", \"val\"}\n",
    "\n",
    "        data_root = Path.cwd()\n",
    "        shards = os.listdir(data_root)\n",
    "        shards = [file for file in shards if \"array\" in file]\n",
    "        shards = sorted(shards)\n",
    "        shards = [os.path.join(data_root, shard) for shard in shards]\n",
    "        self.shards = shards\n",
    "        assert len(shards) > 0, f\"no shards found for split {split}\"\n",
    "        print(f\"found {len(shards)} shards for split {split}\")\n",
    "\n",
    "        self.current_shard = 0\n",
    "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "        self.current_position = self.B * self.T\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        x = (buf[:-1]).view(B, T)\n",
    "        y = (buf[1:]).view(B, T)\n",
    "        self.current_position += B * T\n",
    "\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_shard += 1\n",
    "            self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "            self.current_position = B * T\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DataLoaderLite:\n",
    "#     def __init__(self, B, T):\n",
    "#         self.B = B\n",
    "#         self.T = T\n",
    "\n",
    "#         with open(\"input.txt\", \"r\") as f:\n",
    "#             text = f.read()\n",
    "\n",
    "#         enc = tiktoken.get_encoding(\"gpt2\")\n",
    "#         tokens = enc.encode(text)\n",
    "#         self.tokens = torch.tensor(tokens)\n",
    "#         print(f\"loaded {len(self.tokens)} tokens\")\n",
    "#         print(f\"1 epoch = {len(self.tokens)//(B*T)} batches\")\n",
    "\n",
    "#         self.current_position = 0\n",
    "\n",
    "#     def next_batch(self):\n",
    "#         B, T = self.B, self.T\n",
    "#         buf = self.tokens[self.current_position : self.current_position + B * T + 1]\n",
    "#         x = (buf[:-1]).view(B, T)\n",
    "#         y = (buf[1:]).view(B, T)\n",
    "#         self.current_position += B * T\n",
    "\n",
    "#         if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "#             self.current_position = 0\n",
    "\n",
    "#         return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(it):\n",
    "    if it < warmup_steps:\n",
    "        return max_lr * (it + 1) / warmup_steps\n",
    "\n",
    "    if it > max_steps:\n",
    "        return min_lr\n",
    "\n",
    "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "\n",
    "    return min_lr + coeff * (max_lr - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "# device = \"cpu\"  # override\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "B = 8 # micro batch size\n",
    "T = 1024 # sequence length\n",
    "\n",
    "train_loader = DataLoaderLite(B, T, split=\"train\")\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "model = GPT(GPTConfig(vocab_size=50304))\n",
    "model.to(device)\n",
    "# model = torch.compile(model)\n",
    "\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 300\n",
    "max_steps = 1500\n",
    "# warmup_steps = 3200\n",
    "# max_steps = 8000\n",
    "\n",
    "\n",
    "optimizer = model.configure_optimizers(\n",
    "    weight_decay=0.1, learning_rate=6e-4, device=device\n",
    ")\n",
    "\n",
    "for step in range(max_steps):\n",
    "    t0 = time.time()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    # google colab - Tesla T4 does not support bfloat16 compilation natively\n",
    "    # with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "    #     logits, loss = model(x, y)\n",
    "    logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    dt = (t1 - t0) * 1000\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "    if step % 100 == 0:\n",
    "        print(\n",
    "            f\"step {step+1:4d} | loss: {loss.item():.6f} | norm: {norm:.4f} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec:.2f}\"\n",
    "        )\n",
    "\n",
    "print(f\"final | loss: {loss.item():.6f} | norm: {norm:.4f} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_return_sequences = 10\n",
    "max_length = 100\n",
    "\n",
    "# enc = tiktoken.get_encoding(\"gpt2\")\n",
    "tokens = encode_text(\"Hi, I am a large language model,\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "x = tokens.to(device)\n",
    "\n",
    "# torch.manual_seed(42)\n",
    "# torch.cuda.manual_seed(42)\n",
    "\n",
    "while x.size(1) < max_length:\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)[0]\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        ix = torch.multinomial(topk_probs, 1)\n",
    "        xcol = torch.gather(topk_indices, -1, ix)\n",
    "        x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = x[i, :max_length].tolist()\n",
    "    decoded = decode_tokens(tokens)\n",
    "    print(\">\", decoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
